# <ruby>线程层次<rp>(</rp><rt>Thread Hierarchy</rt><rp>)</rp></ruby>

    `threadIdx`是三维向量，可通过索引来构造线程的一维、二维或三维的块，即线程块。

    线程索引与线程ID的关系：（大小为 (Dx,Dy,Dz)的三维块）

*一维块：索引(x)的ID = x；*

*二维块：索引(x,y)的ID = x+yDx；*

*三维块：索引(x,y,z)的ID = x+yDx+zDxDy；*

> 这个和下面i和j是指x，y，并不是ID，因为计算机上并没有真的有三维块，都是一维数据，通过三维索引转成相应ID找到需要的数据

    以下代码将两个大小为N*N的矩阵A和B相加，并将结果存储到矩阵C中。

```cpp
// Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N],
                       float C[N][N])
{
    int i = threadIdx.x;
    int j = threadIdx.y;
    C[i][j] = A[i][j] + B[i][j];
}

int main()
{
    ...
    // Kernel invocation with one block of N * N * 1 threads
    int numBlocks = 1;
    dim3 threadsPerBlock(N, N);
    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
    ...
}
```

    每个块的线程数量与处理器核心相关，线程块可能最多只有1024个线程，但是内核可以被多个相同形状的线程块运行。所以总的线程数量应该为每个块的线程数乘以块数。

    块被组织成一维、二维或三维的线程块网格，如下图所示。网格中的线程块数量通常由正在处理的数据的大小决定，一般会超过系统中的处理器数量。

<div>
<img title="" src="../第五章%20编程模型/5.2  grid-of-thread-blocks.png" alt="" data-align="inline">
</div>

    ```<<<...>>> ```语法中所指定的每个块的线程数和每个网格的块数的类型为```int```或`dim3`类型。如上例所示，可以指定二维块或网格。

    网格中的每个块都可以由一个具有一维、二维或三维的唯一索引进行识别，该索引可以通过内置的`blockIdx`变量在内核中访问。线程块的维度（尺寸）可以通过内置的`blockDim`变量在内核中访问。

    扩展前面的`MatAdd()`示例对多个块进行处理，代码如下所示。

```cpp
// Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N],
float C[N][N])
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int j = blockIdx.y * blockDim.y + threadIdx.y;
    if (i < N && j < N)    //1
        C[i][j] = A[i][j] + B[i][j];
}

int main()
{
    ...
    // Kernel invocation
    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);  //2//3
    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);    //4
    ...
}
```

1. 内核函数记得要处理边界问题

2. 块的维度是(Dx,Dy)

3. 假设每个维度中每个网格的线程数可以被该维度中每个块的线程数整除

4. `<<<`块的数量或维度、线程的数量或维度=256`>>>`

    线程块需要具备独立执行的能力：必须可以以任何顺序并行或串行执行它们。这种独立性的要求让线程块可以在任意数量的内核之间，以任意顺序来调度，如下图所示，这使程序员能够编写支持处理器核心数量的代码。

<div align="center">
<img title="" src="../第五章%20编程模型/5.2 automatic-scalability.png" alt="" data-align="inline">
</div>

    块内的线程可以进行协作，协作通过使用一些共享内存`(shared memory)`来共享数据或通过同步彼此执行来协调内存访问实现。 更准确地说，可以通过调用`__syncthreads()`内部函数来指定内核中的同步点；`__syncthreads()`充当屏障，块中的所有线程必须等待同步，然后才能继续运行。[Shared Memory]()给出了一个使用共享内存的例子。 除了` __syncthreads()`之外，[Cooperative Groups API]()还提供了一组丰富的线程同步示例。

    为了高效协作，共享内存是每个处理器核心附近的低延迟内存（很像L1缓存），并且`__syncthreads()`是轻量级的。

## <ruby>线程块群集<rp>(</rp><rt>Thread Block Clusters</rt><rp>)</rp></ruby>

> 没那么多卡，过。

## 例子

> 好家伙，朋友你已经学会了加法，请求出矩阵乘法结果吧

矩阵乘法

C[m\*o] = A[m\*n] \* B[n*o]

$$
C = 
\begin{pmatrix} 
  a_{11} & a_{12} \\ 
  a_{21} & a_{22} 
\end{pmatrix}
\begin{pmatrix} 
  b_{11} & b_{12} \\ 
  b_{21} & b_{22} 
\end{pmatrix} = 
\begin{pmatrix} 
  a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\ 
  a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22} 
\end{pmatrix}
$$

```cpp
// System includes
#include <assert.h>
#include <stdio.h>

// CUDA runtime
#include <cuda_profiler_api.h>
#include <cuda_runtime.h>

// Helper functions and utilities to work with CUDA
#include <helper_cuda.h>
#include <helper_functions.h>

/**

* Matrix multiplication (CUDA Kernel) on the device: C = A * B

* wA is A's width and wB is B's width
  */
  template <int BLOCK_SIZE> __global__ void MatrixMulCUDA(float *C, 
    float *A, float *B, int wA, int wB)
  {
   // Block index
   int bx = blockIdx.x;
   int by = blockIdx.y;

   // Thread index
   int tx = threadIdx.x;
   int ty = threadIdx.y;

   // Index of the first sub-matrix of A processed by the block
   int aBegin = wA * BLOCK_SIZE * by;

   // Index of the last sub-matrix of A processed by the block
   int aEnd = aBegin + wA - 1;

   // Step size used to iterate through the sub-matrices of A
   int aStep = BLOCK_SIZE;

   // Index of the first sub-matrix of B processed by the block
   int bBegin = BLOCK_SIZE * bx;

   // Step size used to iterate through the sub-matrices of B
   int bStep = BLOCK_SIZE * wB;

   // Csub is used to store the element of the block sub-matrix
   // that is computed by the thread
   float Csub = 0;

   // Loop over all the sub-matrices of A and B
   // required to compute the block sub-matrix
   for (int a = aBegin, b = bBegin; a <= aEnd; a += aStep, b += bStep) {

       // Declaration of the shared memory array As used to
       // store the sub-matrix of A
       __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];

       // Declaration of the shared memory array Bs used to
       // store the sub-matrix of B
       __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];

       // Load the matrices from device memory
       // to shared memory; each thread loads
       // one element of each matrix
       As[ty][tx] = A[a + wA * ty + tx];
       Bs[ty][tx] = B[b + wB * ty + tx];

       // Synchronize to make sure the matrices are loaded
       __syncthreads();

       // Multiply the two matrices together;
       // each thread computes one element
       // of the block sub-matrix

  #pragma unroll

       for (int k = 0; k < BLOCK_SIZE; ++k) {
           Csub += As[ty][k] * Bs[k][tx];
       }

       // Synchronize to make sure that the preceding
       // computation is done before loading two new
       // sub-matrices of A and B in the next iteration
       __syncthreads();

   }

   // Write the block sub-matrix to device memory;
   // each thread writes one element
   int c               = wB * BLOCK_SIZE * by + BLOCK_SIZE * bx;
   C[c + wB * ty + tx] = Csub;
  }

void ConstantInit(float *data, int size, float val)
{
    for (int i = 0; i < size; ++i) {
        data[i] = val;
    }
}

/**

* Run a simple test of matrix multiplication using CUDA
  */
  int MatrixMultiply(int argc, char **argv, int block_size, 
    const dim3 &dimsA, const dim3 &dimsB)
  {
   // Allocate host memory for matrices A and B
   unsigned int size_A     = dimsA.x * dimsA.y;
   unsigned int mem_size_A = sizeof(float) * size_A;
   float       *h_A;
   checkCudaErrors(cudaMallocHost(&h_A, mem_size_A));
   unsigned int size_B     = dimsB.x * dimsB.y;
   unsigned int mem_size_B = sizeof(float) * size_B;
   float       *h_B;
   checkCudaErrors(cudaMallocHost(&h_B, mem_size_B));
   cudaStream_t stream;

   // Initialize host memory
   const float valB = 0.01f;
   ConstantInit(h_A, size_A, 1.0f);
   ConstantInit(h_B, size_B, valB);

   // Allocate device memory
   float *d_A, *d_B, *d_C;

   // Allocate host matrix C
   dim3         dimsC(dimsB.x, dimsA.y, 1);
   unsigned int mem_size_C = dimsC.x * dimsC.y * sizeof(float);
   float       *h_C;
   checkCudaErrors(cudaMallocHost(&h_C, mem_size_C));

   if (h_C == NULL) {

       fprintf(stderr, "Failed to allocate host matrix C!\n");
       exit(EXIT_FAILURE);

   }

   checkCudaErrors(cudaMalloc(reinterpret_cast<void **>(&d_A), mem_size_A));
   checkCudaErrors(cudaMalloc(reinterpret_cast<void **>(&d_B), mem_size_B));
   checkCudaErrors(cudaMalloc(reinterpret_cast<void **>(&d_C), mem_size_C));
   // Allocate CUDA events that we'll use for timing
   cudaEvent_t start, stop;
   checkCudaErrors(cudaEventCreate(&start));
   checkCudaErrors(cudaEventCreate(&stop));

   checkCudaErrors(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));

   // copy host memory to device
   checkCudaErrors(cudaMemcpyAsync(d_A, h_A, mem_size_A, 
        cudaMemcpyHostToDevice, stream));
   checkCudaErrors(cudaMemcpyAsync(d_B, h_B, mem_size_B,
        cudaMemcpyHostToDevice, stream));

   // Setup execution parameters
   dim3 threads(block_size, block_size);
   dim3 grid(dimsB.x / threads.x, dimsA.y / threads.y);

   // Create and start timer
   printf("Computing result using CUDA Kernel...\n");

   // Performs warmup operation using matrixMul CUDA kernel
   if (block_size == 16) {

       MatrixMulCUDA<16><<<grid, threads, 0, stream>>>(d_C, d_A, 
            d_B, dimsA.x, dimsB.x);

   }
   else {

       MatrixMulCUDA<32><<<grid, threads, 0, stream>>>(d_C, d_A, 
            d_B, dimsA.x, dimsB.x);

   }

   printf("done\n");
   checkCudaErrors(cudaStreamSynchronize(stream));

   // Record the start event
   checkCudaErrors(cudaEventRecord(start, stream));

   // Execute the kernel
   int nIter = 300;

   for (int j = 0; j < nIter; j++) {

       if (block_size == 16) {
           MatrixMulCUDA<16><<<grid, threads, 0, stream>>>(d_C, d_A, 
                d_B, dimsA.x, dimsB.x);
       }
       else {
           MatrixMulCUDA<32><<<grid, threads, 0, stream>>>(d_C, d_A, 
                d_B, dimsA.x, dimsB.x);
       }

   }

   // Record the stop event
   checkCudaErrors(cudaEventRecord(stop, stream));

   // Wait for the stop event to complete
   checkCudaErrors(cudaEventSynchronize(stop));

   float msecTotal = 0.0f;
   checkCudaErrors(cudaEventElapsedTime(&msecTotal, start, stop));

   // Compute and print the performance
   float  msecPerMatrixMul = msecTotal / nIter;
   double flopsPerMatrixMul =

       2.0 * static_cast<double>(dimsA.x) * static_cast<double>(dimsA.y) *
       static_cast<double>(dimsB.x);

   double gigaFlops = (flopsPerMatrixMul * 1.0e-9f) / (msecPerMatrixMul /
       1000.0f);
   printf("Performance= %.2f GFlop/s, Time= %.3f msec, Size= %.0f Ops,"

          " WorkgroupSize= %u threads/block\n",
          gigaFlops,
          msecPerMatrixMul,
          flopsPerMatrixMul,
          threads.x * threads.y);

   // Copy result from device to host
   checkCudaErrors(cudaMemcpyAsync(h_C, d_C, mem_size_C,
         cudaMemcpyDeviceToHost, stream));
   checkCudaErrors(cudaStreamSynchronize(stream));

   printf("Checking computed result for correctness: ");
   bool correct = true;

   // test relative error by the formula
   //     |<x, y>_cpu - <x,y>_gpu|/<|x|, |y|>  < eps
   double eps = 1.e-6; // machine zero

   for (int i = 0; i < static_cast<int>(dimsC.x * dimsC.y); i++) {

       double abs_err    = fabs(h_C[i] - (dimsA.x * valB));
       double dot_length = dimsA.x;
       double abs_val    = fabs(h_C[i]);
       double rel_err    = abs_err / abs_val / dot_length;

       if (rel_err > eps) {
           printf("Error! Matrix[%05d]=%.8f, ref=%.8f error term is > %E\n", i, h_C[i], dimsA.x * valB, eps);
           correct = false;
       }

   }

   printf("%s\n", correct ? "Result = PASS" : "Result = FAIL");

   // Clean up memory
   checkCudaErrors(cudaFreeHost(h_A));
   checkCudaErrors(cudaFreeHost(h_B));
   checkCudaErrors(cudaFreeHost(h_C));
   checkCudaErrors(cudaFree(d_A));
   checkCudaErrors(cudaFree(d_B));
   checkCudaErrors(cudaFree(d_C));
   checkCudaErrors(cudaEventDestroy(start));
   checkCudaErrors(cudaEventDestroy(stop));
   printf("\nNOTE: The CUDA Samples are not meant for performance "

          "measurements. Results may vary when GPU Boost is enabled.\n");

   if (correct) {

       return EXIT_SUCCESS;

   }
   else {

       return EXIT_FAILURE;

   }
  }

/**

* Program main
  */
  int main(int argc, char **argv)
  {
   printf("[Matrix Multiply Using CUDA] - Starting...\n");

   if (checkCmdLineFlag(argc, (const char **)argv, "help") || checkCmdLineFlag(argc, (const char **)argv, "?")) {

       printf("Usage -device=n (n >= 0 for deviceID)\n");
       printf("      -wA=WidthA -hA=HeightA (Width x Height of Matrix A)\n");
       printf("      -wB=WidthB -hB=HeightB (Width x Height of Matrix B)\n");
       printf("  Note: Outer matrix dimensions of A & B matrices"
              " must be equal.\n");

       exit(EXIT_SUCCESS);

   }

   // This will pick the best possible CUDA capable device, otherwise
   // override the device ID based on input provided at the command line
   int dev = findCudaDevice(argc, (const char **)argv);

   int block_size = 32;

   dim3 dimsA(5 * 2 * block_size, 5 * 2 * block_size, 1);
   dim3 dimsB(5 * 4 * block_size, 5 * 2 * block_size, 1);

   // width of Matrix A
   if (checkCmdLineFlag(argc, (const char **)argv, "wA")) {

       dimsA.x = getCmdLineArgumentInt(argc, (const char **)argv, "wA");

   }

   // height of Matrix A
   if (checkCmdLineFlag(argc, (const char **)argv, "hA")) {

       dimsA.y = getCmdLineArgumentInt(argc, (const char **)argv, "hA");

   }

   // width of Matrix B
   if (checkCmdLineFlag(argc, (const char **)argv, "wB")) {

       dimsB.x = getCmdLineArgumentInt(argc, (const char **)argv, "wB");

   }

   // height of Matrix B
   if (checkCmdLineFlag(argc, (const char **)argv, "hB")) {

       dimsB.y = getCmdLineArgumentInt(argc, (const char **)argv, "hB");

   }

   if (dimsA.x != dimsB.y) {

       printf("Error: outer matrix dimensions must be equal. (%d != %d)\n", dimsA.x, dimsB.y);
       exit(EXIT_FAILURE);

   }

   printf("MatrixA(%d,%d), MatrixB(%d,%d)\n", dimsA.x, dimsA.y, dimsB.x, dimsB.y);

   checkCudaErrors(cudaProfilerStart());
   int matrix_result = MatrixMultiply(argc, argv, block_size, 
        dimsA, dimsB);
   checkCudaErrors(cudaProfilerStop());

   exit(matrix_result);
  }
```

### 内核函数定义

```cpp
  template <int BLOCK_SIZE> __global__ void MatrixMulCUDA(float *C, 
    float *A, float *B, int wA, int wB)
```

`template <int BLOCK_SIZE>`开局使用模板减少代码复写，没放到参数是给编译时就确定`BLOCK_SIZE`，减少程序运行时负担。

BLOCK_SIZE——正方形块的大小

wA——矩阵A宽

wB——矩阵B宽，也是矩阵A长，B宽和A长必须一致才能计算

```cpp
  {
   // Block index
   int bx = blockIdx.x;
   int by = blockIdx.y;

   // Thread index
   int tx = threadIdx.x;
   int ty = threadIdx.y;

   // Index of the first sub-matrix of A processed by the block
   int aBegin = wA * BLOCK_SIZE * by;

   // Index of the last sub-matrix of A processed by the block
   int aEnd = aBegin + wA - 1;

   // Step size used to iterate through the sub-matrices of A
   int aStep = BLOCK_SIZE;

   // Index of the first sub-matrix of B processed by the block
   int bBegin = BLOCK_SIZE * bx;

   // Step size used to iterate through the sub-matrices of B
   int bStep = BLOCK_SIZE * wB;
  // Csub is used to store the element of the block sub-matrix
   // that is computed by the thread
   float Csub = 0;

   // Loop over all the sub-matrices of A and B
   // required to compute the block sub-matrix
   for (int a = aBegin, b = bBegin; a <= aEnd; a += aStep, b += bStep) {
```

    矩阵分为子矩阵运算。

```cpp
       // Declaration of the shared memory array As used to
       // store the sub-matrix of A
       __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];

       // Declaration of the shared memory array Bs used to
       // store the sub-matrix of B
       __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];

       // Load the matrices from device memory
       // to shared memory; each thread loads
       // one element of each matrix
       As[ty][tx] = A[a + wA * ty + tx];
       Bs[ty][tx] = B[b + wB * ty + tx];

       // Synchronize to make sure the matrices are loaded
       __syncthreads();

       // Multiply the two matrices together;
       // each thread computes one element
       // of the block sub-matrix

  #pragma unroll

       for (int k = 0; k < BLOCK_SIZE; ++k) {
           Csub += As[ty][k] * Bs[k][tx];
       }

       // Synchronize to make sure that the preceding
       // computation is done before loading two new
       // sub-matrices of A and B in the next iteration
       __syncthreads();

   }

   // Write the block sub-matrix to device memory;
   // each thread writes one element
   int c               = wB * BLOCK_SIZE * by + BLOCK_SIZE * bx;
   C[c + wB * ty + tx] = Csub;
  }
```

`__shared__`——共享内存，与线程块同生命周期，类似CPU的L1级缓存

`__syncthreads()`——同步用，类似水库的闸口，山上的水全汇到一起再开闸

`#pragma unroll`——让编译器对循环体不展开，不增加代码量；例如循环100次打印，编译器优化可能写100行打印代码，而不是运行时再循环

<div>
<img title="" src="../第五章%20编程模型/5.2 线程层次附图1.png" alt="" data-align="inline">
</div>

    按图来说在下如何安排的：

假设线程块[0,0]开始运算（其他块也在同理运算），

1. 块[0,0]涉及到A矩阵[0]-[30]，B矩阵[1]、[11]...[93]的计算
2. 线程[0,0]第一次循环获得A[1]和B[1]，线程[0,1]获得A[2]和B[2]，...，线程[2,2]获得A[23]和B[23]
3. 同步赋值完成
4. 计算得到线程[0,0]计算As[0,0~2]与Bs[0~2,0]积之和...即子矩阵乘法
5. 同步计算C块[0,0]第一次循环完成
6. 线程[0,0]第二次循环获得A[4]和B[31]，线程[0,1]获得A[5]和B[32]，...，线程[2,2]获得A[26]和B[53]
7. ...（累加）

对于一个线程中Csub就统计A的一行和B的一列的积之和，统计完成把Csub赋值给C块对应位置就好
